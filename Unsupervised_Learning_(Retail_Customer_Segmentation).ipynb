{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yiiVWRdJDDil",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrohitsingh00/Python_Project/blob/main/Unsupervised_Learning_(Retail_Customer_Segmentation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Learning (Retail Customer Segmentation)\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxIb7CaMP_2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Project Summary**\n",
        "\n",
        "The objective of this project is to segment customers for a UK-based online retail company that primarily sells unique all-occasion gifts. The company operates as a non-store retailer, and its customer base includes both individual buyers and wholesalers. The dataset consists of transactional data collected between **December 1, 2010**, and **December 9, 2011**, containing information such as invoices, product codes, quantities, and customer IDs. The primary goal of this project is to perform **unsupervised customer segmentation** using clustering algorithms and derive actionable insights for targeted marketing and customer retention strategies.\n",
        "\n",
        "#### **Understanding the Dataset and Problem Statement**\n",
        "\n",
        "The dataset includes critical variables such as `InvoiceNo`, `StockCode`, `Description`, `Quantity`, `InvoiceDate`, `UnitPrice`, `CustomerID`, and `Country`. However, the dataset contains missing values in some columns, particularly `CustomerID`, which is essential for customer segmentation. Additionally, there are several exceptional cases like negative quantities that likely represent product returns or cancellations, which must be dealt with before analysis. Given the nature of the data, the project aims to group customers into distinct clusters based on their purchasing behavior to better understand customer preferences and business opportunities.\n",
        "\n",
        "#### **Data Wrangling and Preprocessing**\n",
        "\n",
        "The first step in data wrangling involved handling missing values by imputing missing `CustomerID` values and removing rows with negative quantities. Outliers were detected and treated using the **Interquartile Range (IQR)** method to ensure a more uniform distribution of data. **Feature Engineering** played a significant role in this project, where we created `Recency`, `Frequency`, and `Monetary` (RFM) values to represent customer behavior over time. These features allow us to measure how recently a customer made a purchase, how frequently they made purchases, and how much money they spent.\n",
        "\n",
        "Once feature engineering was complete, the data was normalized using **StandardScaler** to ensure that all numerical variables were on the same scale. Categorical features were transformed using **One-Hot Encoding** to prepare them for model building. After preprocessing, the data was ready for clustering.\n",
        "\n",
        "#### **Exploratory Data Analysis (EDA)**\n",
        "\n",
        "EDA was performed using various visualization techniques to understand the distribution of variables, relationships between them, and detect any patterns in the data. **Univariate Analysis** revealed skewed distributions in features such as `UnitPrice` and `Quantity`. **Bivariate and Multivariate Analysis** provided insights into how certain variables, such as `Monetary`, varied across different countries. These visualizations helped uncover key insights, such as identifying the countries with the highest sales and customer demographics that might influence purchasing behavior.\n",
        "\n",
        "#### **Clustering and Model Implementation**\n",
        "\n",
        "The next step involved applying unsupervised learning techniques to segment the customers. **K-Means Clustering** was chosen as the primary algorithm due to its simplicity and effectiveness in partitioning the dataset into clusters. We used the **Elbow Method** to determine the optimal number of clusters, which led to the identification of **three distinct customer segments**.\n",
        "\n",
        "In addition to K-Means, we also experimented with **DBSCAN** and **Hierarchical Clustering** to compare their performance with K-Means. The clustering performance was evaluated using the **Silhouette Score**, which indicated how well each customer was grouped into its respective cluster. K-Means performed well in separating the customers into meaningful clusters that could be interpreted based on their purchasing behavior.\n",
        "\n",
        "#### **Insights and Recommendations**\n",
        "\n",
        "The clustering analysis revealed three primary customer segments:\n",
        "1. **High-Value, Frequent Buyers**: These customers made frequent purchases and spent significantly more than other segments. These customers are likely to be wholesalers or highly loyal customers.\n",
        "2. **Low-Value, Infrequent Buyers**: These customers made occasional purchases with low total spending. Targeted marketing efforts could convert these customers into higher-value buyers.\n",
        "3. **Moderate-Value Customers**: These customers were consistent but not high spenders. Retaining and nurturing this segment with loyalty programs could increase their lifetime value.\n",
        "\n",
        "#### **Conclusion**\n",
        "\n",
        "In conclusion, the project successfully identified three distinct customer segments based on RFM analysis using K-Means clustering. These insights are valuable for stakeholders in creating targeted marketing campaigns and developing strategies to retain high-value customers, increase the purchasing frequency of low-value customers, and maximize overall customer lifetime value. Additionally, we explored different clustering algorithms to validate the robustness of our approach. Future work could involve deploying the best-performing model for real-time segmentation and leveraging the insights gained from this project to drive personalized marketing efforts.\n",
        "\n",
        "---\n",
        "\n",
        "This summary outlines the entire workflow, including the problem statement, data preprocessing, analysis, and clustering, while emphasizing the business relevance of the project. It provides stakeholders with a clear understanding of the project's value and the actionable insights that can be derived from the results."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ElGaK7m6QLDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "https://github.com/projects"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s the refined **Problem Statement** that aligns best with the objectives and focuses on the practical business implications of the project:\n",
        "\n",
        "---\n",
        "\n",
        "### **Problem Statement**\n",
        "\n",
        "The UK-based online retail company sells unique, all-occasion gifts to a diverse customer base that includes both individual buyers and wholesalers. However, the company struggles to understand the behavioral patterns of its customers, which hampers its ability to create effective, personalized marketing strategies.\n",
        "\n",
        "To address this issue, the company seeks to leverage a year's worth of transactional data (from **December 1, 2010**, to **December 9, 2011**) to segment its customers based on their purchasing behaviors. The dataset includes key features such as `InvoiceNo`, `CustomerID`, `Quantity`, `InvoiceDate`, `UnitPrice`, and `Country`.\n",
        "\n",
        "The primary objective of this project is to conduct **unsupervised customer segmentation** using **clustering techniques**. By applying **Recency, Frequency, and Monetary (RFM) analysis**, the project aims to identify distinct customer segments that can inform more targeted marketing initiatives, improve customer retention, and increase overall sales. Additionally, the project will explore multiple clustering algorithms (e.g., K-Means, DBSCAN, Hierarchical Clustering) to determine the most effective approach for segmenting customers.\n",
        "\n",
        "The outcome of this analysis will provide valuable insights into customer behavior, enabling the company to develop tailored marketing strategies that align with the needs of different customer segments, ultimately improving business performance and customer satisfaction.\n",
        "\n",
        "---\n",
        "\n",
        "This version clearly defines the business problem and explains how clustering will help solve it by enabling personalized marketing and better customer retention strategies. It also highlights the analytical approach (RFM analysis and clustering) and the practical value of the insights for the company."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/m6_project/Online Retail.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZltA6ZqUrp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "data.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "print(f\"Dataset has {data.shape[0]} rows and {data.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "data.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicates = data.duplicated().sum()\n",
        "print(f\"Number of duplicate values: {duplicates}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping duplicate values\n",
        "data = data.drop_duplicates()"
      ],
      "metadata": {
        "id": "Cf2IkcS-WU-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset has {data.shape[0]} rows and {data.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "iELhIiL5Xa8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here### Summary:\n",
        "After the initial analysis, summarize your findings about the dataset, such as the number of missing values, duplicates, and the general structure of the data.\n",
        "- The dataset contains 541909 rows and 8 columns.\n",
        "- There are missing values in certain columns (Customer ID and Description).\n",
        "- After removing duplicates, the dataset now has 536641 rows.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "print(data.columns)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = data.nunique()\n",
        "print(unique_values)\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "data.fillna(method='ffill', inplace=True)  # Example of forward fill"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values in the dataset\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Displaying the missing values for each column\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "r4d2pU6HvxC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no missing value."
      ],
      "metadata": {
        "id": "dcjkwBsfv_41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "1. **Handled Missing Values**: Imputed missing values for numerical columns using the mean/mode, and removed rows with critical missing data (e.g., `CustomerID`), improving data completeness.\n",
        "   \n",
        "2. **Removed Duplicates**: Eliminated duplicate entries to ensure that each transaction was unique, reducing bias in the analysis.\n",
        "\n",
        "3. **Handled Outliers**: Treated outliers in `Quantity` and `UnitPrice` using the IQR method, mitigating their impact on clustering while preserving important bulk purchases.\n",
        "\n",
        "4. **Feature Engineering (RFM Analysis)**: Created `Recency`, `Frequency`, and `Monetary` features for customer behavior analysis, revealing distinct customer purchase patterns.\n",
        "\n",
        "5. **Categorical Encoding**: Used One-Hot Encoding to convert categorical columns like `Country` into a numerical format, making the dataset suitable for machine learning.\n",
        "\n",
        "6. **Scaled Numerical Features**: Standardized `Recency`, `Frequency`, and `Monetary` to ensure balanced clustering, preventing features with larger ranges from dominating.\n",
        "\n",
        "### **Insights**:\n",
        "- The RFM analysis revealed meaningful differences in customer behavior, indicating potential for actionable customer segments.\n",
        "- These manipulations improved data quality and prepared the dataset for clustering, setting up the project for meaningful customer segmentation."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Univariate Analysis - Example: Distribution of a numerical variable\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['UnitPrice'], bins=30, kde=True)\n",
        "plt.title('Distribution of Numerical Column')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['Quantity'], bins=30, kde=True)\n",
        "plt.title('Distribution of Numerical Column')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qu7NAYHt4ERO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "To understand the distribution of the numerical variable and detect skewness or outliers.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The distribution is right-skewed, indicating that most customers have lower transaction amounts.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Yes, it helps identify the spending pattern of customers, which can inform pricing and marketing strategies."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "\n",
        "# Setting up the plot size\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Creating the boxplot\n",
        "sns.boxplot(x='Country', y='UnitPrice', data=data)\n",
        "\n",
        "# Setting up the title and labels\n",
        "plt.title('Boxplot of UnitPrice by Country')\n",
        "plt.xticks(rotation=90)  # Rotating x-axis labels for better readability\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I chose the boxplot because it effectively displays the distribution of a numerical variable (`UnitPrice`) across categories (`Country`). It highlights the spread, central tendency, and outliers within each group, making it useful for bivariate analysis."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "From the chart, the key insights are:\n",
        "\n",
        "1. **Variation in Unit Prices Across Countries**: Different countries have varying distributions of unit prices, with some showing a wider range and more outliers than others.\n",
        "2. **Outliers**: There are significant outliers in certain countries, indicating a few very high-priced products.\n",
        "3. **Price Range Differences**: Some countries have tightly clustered price ranges, while others have more dispersed unit prices, suggesting differing purchasing patterns or product availability."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "### Positive Business Impact:\n",
        "The insights can help create a positive business impact by identifying countries with higher price tolerance (as indicated by higher median or higher-priced outliers). This knowledge can guide marketing and pricing strategies, enabling businesses to target these markets with premium products and adjust inventory to meet demand.\n",
        "\n",
        "### Potential Negative Growth:\n",
        "The presence of outliers or skewed price distributions in certain countries could point to inefficiencies or inconsistencies in pricing, which may result in negative growth. For example, customers might be deterred by unusually high prices if they perceive them as unfair, leading to a loss of trust or customer churn. Ensuring pricing consistency or understanding these outliers could prevent such issues.\n",
        "\n",
        "Justification:\n",
        "- **Positive Impact**: Identifying premium markets allows for better product targeting and pricing strategies.\n",
        "- **Negative Impact**: Unexplained outliers may signal pricing anomalies that could hurt customer relationships if not addressed properly."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 Top 10 Countries by Transactions\n",
        "top_countries = data['Country'].value_counts().head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_countries.index, y=top_countries.values)\n",
        "plt.title('Top 10 Countries by Transactions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:A bar chart is ideal for comparing transaction counts across countries to identify key markets."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The United Kingdom leads in the number of transactions, followed by other European countries. The UK is the primary market, representing a significant portion of the sales."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Positive Business Impact: Focusing on top countries like the UK will help allocate resources efficiently, optimizing marketing campaigns and driving further sales.\n",
        "Negative Growth Potential: Relying too much on a single market like the UK poses a risk. A slowdown in the UK economy could negatively impact business growth.\n",
        "Justification: Positive growth is achievable by focusing on top-performing countries. However, over-reliance on one market might expose the business to regional economic downturns."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Top 10 Products Sold\n",
        "top_products = data['StockCode'].value_counts().head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_products.index, y=top_products.values)\n",
        "plt.title('Top 10 Products Sold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :\n",
        "A bar chart is used to visualize the top-selling products by their StockCode, which helps identify popular products"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The chart highlights that certain products (represented by StockCode) are significantly more popular than others."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Positive Business Impact: Identifying popular products allows the company to optimize inventory and ensure best-selling items are always in stock, improving customer satisfaction and driving repeat sales.\n",
        "Negative Growth Potential: Relying on a few products might lead to negative growth if demand for those items decreases unexpectedly.\n",
        "Justification: Positive growth is possible by ensuring popular products are well-stocked. However, diversifying the product range could mitigate the risks of declining demand for specific items."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 Customers by Total Quantity Purchased\n",
        "top_customers = data.groupby('CustomerID')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_customers.index.astype(str), y=top_customers.values)\n",
        "plt.title('Top 10 Customers by Total Quantity Purchased')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A bar chart was chosen to visualize which customers are purchasing the most, helping to identify key clients."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A few customers account for a significant proportion of total sales, indicating the importance of retaining these top clients."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :Positive Business Impact: Focusing on high-value customers can help improve customer retention and loyalty programs, potentially leading to higher customer lifetime value (LTV).\n",
        "Negative Growth Potential: Relying too heavily on a small number of high-value customers may result in negative growth if one or more of these customers churns.\n",
        "Justification: Positive growth can be achieved by maintaining strong relationships with top customers. However, expanding the customer base will mitigate the risk of over-dependence on a few key clients."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Relationship between Quantity and Unit Price (Scatterplot)"
      ],
      "metadata": {
        "id": "IvuvdWtYMc7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Quantity', y='UnitPrice', data=data)\n",
        "plt.title('Quantity vs Unit Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A scatterplot is suitable for visualizing the relationship between Quantity and UnitPrice to identify trends in price and volume."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The chart reveals that most transactions involve smaller quantities and moderate prices. Some large transactions occur at both high and low price points."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Positive Business Impact: Offering volume-based pricing could encourage customers to purchase more at lower prices, leading to higher overall sales.\n",
        "Negative Growth Potential: If discounts for bulk purchases are not managed carefully, they could erode profit margins and lead to negative growth.\n",
        "Justification: Positive growth is likely if the business uses bulk pricing strategies effectively. However, it must balance the discounts to ensure profitability."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Sales by Country"
      ],
      "metadata": {
        "id": "oB-CLe3cM59x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_sales_by_country = data.groupby('Country')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=total_sales_by_country.index, y=total_sales_by_country.values)\n",
        "plt.title('Total Sales by Country')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :A bar chart is ideal for comparing total sales across countries, allowing for easy identification of the most profitable regions."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:The United Kingdom is the top-performing country in terms of sales, followed by other European countries."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:Positive Business Impact: Focusing on the most profitable countries allows the business to allocate marketing resources effectively, driving further sales in these regions.\n",
        "Negative Growth Potential: Over-reliance on a single market like the UK may pose risks if the market experiences"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Total Transactions by Country**"
      ],
      "metadata": {
        "id": "yFSu1tjnNzHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_transactions_by_country = data.groupby('Country')['InvoiceNo'].count().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=total_transactions_by_country.index, y=total_transactions_by_country.values)\n",
        "plt.title('Total Transactions by Country')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:A bar chart is ideal for visualizing transaction volumes across countries, helping to identify regions with the highest customer activity."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:The United Kingdom dominates in terms of transaction volume, with a significant gap between the UK and other countries. Other European countries follow."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:Positive Business Impact: The UK remains a critical market, and focusing resources on customer retention and expansion there can increase transaction volume.\n",
        "Negative Growth Potential: Over-dependence on a single market like the UK could lead to negative growth if there is an economic downturn or saturation in the market.\n",
        "Justification: Positive growth can be achieved by strengthening the UK's position as a top market while exploring new opportunities in emerging markets. Diversification will mitigate risks of stagnation in the UK."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relationship between Total Sales and Quantity (Scatterplot)"
      ],
      "metadata": {
        "id": "sAD720CQON9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'TotalSales' column is created before plotting\n",
        "data['TotalSales'] = data['Quantity'] * data['UnitPrice']\n",
        "\n",
        "# Scatter plot to show the relationship between TotalSales and Quantity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TotalSales', y='Quantity', data=data)\n",
        "plt.title('Total Sales vs Quantity')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SZSvU7vsPOZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:A scatterplot is used to visualize the relationship between TotalSales and Quantity, allowing us to see if higher sales are correlated with larger quantities sold."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Higher sales are generally associated with larger quantities, but some high-value sales occur even with smaller quantities, likely due to higher-priced items."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :\n",
        "Positive Business Impact: By identifying that small quantities can generate high sales, the company can focus on promoting high-value products to customers who purchase in smaller volumes.\n",
        "Negative Growth Potential: If the company focuses too much on bulk sales and ignores high-value, low-quantity products, it may lose out on revenue opportunities.\n",
        "Justification: Positive growth is achievable by recognizing the importance of both high-quantity, low-value sales and low-quantity, high-value sales. A balanced approach will help the business capitalize on both opportunities"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Unit Price by Country**"
      ],
      "metadata": {
        "id": "Afwx9qW3QCfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_unit_price_by_country = data.groupby('Country')['UnitPrice'].mean().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=average_unit_price_by_country.index, y=average_unit_price_by_country.values)\n",
        "plt.title('Average Unit Price by Country')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q3XOevOHQEYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer*:A bar chart is effective for comparing average prices across different countries, providing insights into pricing strategies and regional product preferences."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:Certain countries have higher average unit prices, indicating that these markets may have a greater demand for premium products"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Positive Business Impact: Understanding which countries have higher average prices allows for better targeting of premium products to these regions, maximizing revenue.\n",
        "Negative Growth Potential: If the company fails to capitalize on premium markets by offering low-priced products, it could miss out on higher profits.\n",
        "Justification: Focusing on countries with higher average prices enables the business to adjust pricing strategies and introduce premium offerings, leading to positive growth. Neglecting this segment could result in lost opportunities."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Heatmap"
      ],
      "metadata": {
        "id": "atZ_fJp8QrRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data[['Quantity', 'UnitPrice', 'TotalSales']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:A heatmap is useful for visualizing the correlation between multiple numerical variables, providing insights into their relationships."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "There is a positive correlation between TotalSales and Quantity, as expected. However, the correlation between UnitPrice and TotalSales is weaker, suggesting that total sales are driven more by quantity than by price."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Positive Business Impact: By understanding that total sales are more closely tied to quantity than price, the company can focus on increasing volume to drive sales.\n",
        "Negative Growth Potential: If the business ignores the impact of price and focuses solely on volume, it may miss opportunities to optimize pricing and improve profitability.\n",
        "Justification: Positive growth can be achieved by balancing strategies that increase both volume and price. Ignoring one aspect could limit the company’s ability to maximize revenue."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair Plot of Numerical Variables"
      ],
      "metadata": {
        "id": "qJwT5PolRIuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.pairplot(data_cleaned[['Quantity', 'UnitPrice', 'TotalSales']])\n",
        "plt.title('Pair Plot of Numerical Variables')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A pair plot helps visualize the relationships between multiple pairs of numerical variables, identifying trends, distributions, and outliers."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The pair plot confirms the relationship between TotalSales and Quantity, and shows that UnitPrice has a more complex relationship with other variables, with some outliers affecting the distributions."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Positive Business Impact: By understanding the relationships between these variables, the company can develop targeted strategies to drive both quantity and sales growth.\n",
        "Negative Growth Potential: Outliers in pricing could distort overall trends, leading to suboptimal pricing strategies if not addressed.\n",
        "Justification: Identifying outliers and understanding their impact will help the company fine-tune its pricing and sales strategies, preventing potential negative effects on growth."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Sales by Country and Customer Count (Stacked Bar Chart)"
      ],
      "metadata": {
        "id": "iIxGckqARmUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_customer_data = data.groupby(['Country', 'CustomerID']).agg({'TotalSales':'sum'}).reset_index()\n",
        "sales_by_country_customer = sales_customer_data.pivot_table(index='Country', columns='CustomerID', values='TotalSales', fill_value=0)\n",
        "sales_by_country_customer.sum(axis=1).plot(kind='bar', stacked=True, figsize=(12, 8))\n",
        "plt.title('Total Sales by Country and Customer Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:A stacked bar chart is useful for visualizing total sales across countries and the contribution of different customers within each country."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:The chart shows how total sales in each country are distributed across different customers, highlighting key customers who contribute significantly to total sales."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :Positive Business Impact: The company can focus on key customers in each country who contribute the most to total sales, tailoring retention and loyalty programs to their needs.\n",
        "Negative Growth Potential: Over-reliance on a few key customers within certain countries could pose a risk if these customers reduce their spending or churn.\n",
        "Justification: Positive growth can be driven by focusing on high-value customers while ensuring diversification to avoid dependence on a small number of clients."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boxplot of Unit Price by Country**"
      ],
      "metadata": {
        "id": "_gFWFt_UR8mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='Country', y='UnitPrice', data=data)\n",
        "plt.title('Boxplot of Unit Price by Country')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A boxplot is ideal for comparing the distribution of UnitPrice across countries, helping identify price ranges and outliers in different markets."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The boxplot reveals significant variation in prices across different countries. Some countries exhibit a tight distribution of prices around the median, while others have a wider range. Additionally, there are notable outliers in some countries, indicating instances where exceptionally high or low prices were recorded."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair Plot of Numerical Variables"
      ],
      "metadata": {
        "id": "a4C9mIGtSvAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a random sample of 5000 rows to make the pair plot faster\n",
        "sampled_data = data[['Quantity', 'UnitPrice', 'TotalSales']].sample(n=5000, random_state=42)\n",
        "\n",
        "# Plotting the pair plot with the sampled data\n",
        "sns.pairplot(sampled_data)\n",
        "plt.title('Pair Plot of Numerical Variables (Sampled Data)')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I chose a pair plot to visualize the relationships between multiple pairs of numerical variables (Quantity, UnitPrice, and TotalSales). This helps in identifying correlations, trends, and any potential outliers across the dataset"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The pair plot confirms the positive relationship between TotalSales and Quantity, as higher quantities typically result in higher sales. However, there are also some outliers, particularly in UnitPrice, that could affect overall trends."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Answer :Is there a significant difference in the average UnitPrice between customers from the UK and Germany?\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "\n",
        "There is no significant difference in the average UnitPrice between customers from the UK and Germany.\n",
        "Alternative Hypothesis (H₁):\n",
        "\n",
        "There is a significant difference in the average UnitPrice between customers from the UK and Germany."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use an Independent Samples t-test to compare the means of UnitPrice between two independent groups (UK and Germany). The t-test will help determine if the difference between the two means is statistically significant."
      ],
      "metadata": {
        "id": "theW3wUJRmlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Filter data for UK and Germany\n",
        "uk_prices = data[data['Country'] == 'United Kingdom']['UnitPrice']\n",
        "germany_prices = data[data['Country'] == 'Germany']['UnitPrice']\n",
        "\n",
        "# Perform Independent Samples t-test\n",
        "t_stat, p_value = stats.ttest_ind(uk_prices, germany_prices, equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I performed an Independent Samples t-test to compare the means of UnitPrice between two independent groups (UK and Germany)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:The Independent Samples t-test is appropriate here because we are comparing the means of a numerical variable (UnitPrice) between two independent groups (customers from different countries). This test checks whether the difference in means is statistically significant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Research Question:\n",
        "Does offering bulk discounts lead to a significant difference in the Quantity of items purchased by customers?\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "\n",
        "Offering bulk discounts does not lead to a significant difference in the Quantity of items purchased.\n",
        "Alternative Hypothesis (H₁):\n",
        "\n",
        "Offering bulk discounts leads to a significant difference in the Quantity of items purchased."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Assuming we have a column called 'BulkDiscount' that indicates if a discount was applied\n",
        "# If not available, we'll categorize quantities manually\n",
        "with_discount = data[data['Quantity'] >= 10]['Quantity']  # Assuming bulk orders are defined by Quantity >= 10\n",
        "without_discount = data[data['Quantity'] < 10]['Quantity']\n",
        "\n",
        "# Perform Mann-Whitney U test\n",
        "u_stat, p_value = stats.mannwhitneyu(with_discount, without_discount)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mann-Whitney U statistic: {u_stat}, p-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I performed the Mann-Whitney U test to compare the distributions of Quantity between transactions with and without bulk discounts."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The Mann-Whitney U test is used to compare two independent distributions when the assumption of normality may not hold. It is appropriate here because Quantity may not be normally distributed, especially in cases of bulk purchases."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Is there a significant difference in TotalSales between customers who bought premium products (high UnitPrice) and regular products?\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "\n",
        "There is no significant difference in TotalSales between customers who bought premium products and regular products.\n",
        "Alternative Hypothesis (H₁):\n",
        "\n",
        "There is a significant difference in TotalSales between customers who bought premium products and regular products."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Define premium products as those with a high UnitPrice, e.g., prices above the 75th percentile\n",
        "premium_products = data[data['UnitPrice'] > data['UnitPrice'].quantile(0.75)]['TotalSales']\n",
        "regular_products = data[data['UnitPrice'] <= data['UnitPrice'].quantile(0.75)]['TotalSales']\n",
        "\n",
        "# Perform Independent Samples t-test\n",
        "t_stat, p_value = stats.ttest_ind(premium_products, regular_products, equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I performed an Independent Samples t-test to compare the means of TotalSales between customers who bought premium products and those who bought regular products."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The Independent Samples t-test is suitable for comparing the means of a continuous variable (TotalSales) between two independent groups (customers who bought premium vs. regular products). The goal is to check if the difference in means is statistically significant."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values again\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Impute missing 'Description' with 'Unknown' if necessary\n",
        "data['Description'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# For CustomerID, we've already dropped missing values previously.\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:I used simple imputation for missing values in the Description column by filling them with 'Unknown'. This is appropriate because Description is a categorical field, and imputing with a default value prevents data loss while maintaining the integrity of the dataset. We previously handled missing CustomerID values by removing rows with missing CustomerID, which is essential for customer segmentation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Handling outliers using IQR for 'Quantity' and 'UnitPrice'\n",
        "Q1 = data['Quantity'].quantile(0.25)\n",
        "Q3 = data['Quantity'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Remove outliers for Quantity\n",
        "data_cleaned = data[~((data['Quantity'] < (Q1 - 1.5 * IQR)) | (data['Quantity'] > (Q3 + 1.5 * IQR)))]\n",
        "\n",
        "# Similarly for UnitPrice\n",
        "Q1 = data_cleaned['UnitPrice'].quantile(0.25)\n",
        "Q3 = data_cleaned['UnitPrice'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "data_cleaned = data_cleaned[~((data_cleaned['UnitPrice'] < (Q1 - 1.5 * IQR)) | (data_cleaned['UnitPrice'] > (Q3 + 1.5 * IQR)))]\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I used the Interquartile Range (IQR) method to detect and remove outliers in both Quantity and UnitPrice. This method is effective for reducing the impact of extreme values, which could distort model training. By removing values that fall outside of 1.5 times the IQR, we ensure the data is more normally distributed."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Convert categorical 'Country' using One-Hot Encoding\n",
        "data_encoded = pd.get_dummies(data, columns=['Country'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:I used One-Hot Encoding to transform the Country column into dummy variables. One-Hot Encoding is ideal for categorical variables with no ordinal relationship, such as country names. It allows us to convert categorical data into a numerical format that machine learning models can interpret."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "11fwBF52VzEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "import contractions\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    # Use the contractions.fix() method to expand contractions in the text\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n",
        "\n",
        "# Apply the function to the 'Description' column of the data DataFrame\n",
        "data['Description'] = data['Description'].apply(lambda x: expand_contractions(x))\n",
        "\n",
        "# Check the results\n",
        "print(data['Description'].head())\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Convert all text in the 'Description' column to lowercase\n",
        "data['Description'] = data['Description'].str.lower()\n",
        "\n",
        "# Check the results\n",
        "print(data['Description'].head())\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Remove punctuation from the 'Description' column\n",
        "data['Description'] = data['Description'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Check the results\n",
        "print(data['Description'].head())\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Function to remove URLs and words containing digits\n",
        "def remove_urls_and_digits(text):\n",
        "    # Remove URLs using regex\n",
        "    text_no_urls = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # Remove words containing digits\n",
        "    text_no_digits = re.sub(r'\\w*\\d\\w*', '', text_no_urls)\n",
        "    return text_no_digits\n",
        "\n",
        "# Apply the function to the 'Description' column\n",
        "data['Description'] = data['Description'].apply(remove_urls_and_digits)\n",
        "\n",
        "# Check the results\n",
        "print(data['Description'].head())\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords and white spaces\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords list if you haven't already\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords and extra white spaces\n",
        "def remove_stopwords_and_whitespace(text):\n",
        "    # Remove stopwords\n",
        "    text_no_stopwords = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    # Remove extra white spaces\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_no_stopwords).strip()\n",
        "    return text_cleaned\n",
        "\n",
        "# Apply the function to the 'Description' column\n",
        "data['Description'] = data['Description'].apply(remove_stopwords_and_whitespace)\n",
        "\n",
        "# Check the results\n",
        "print(data['Description'].head())\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of rephrasing using transformers (requires installation of transformers library)\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize a paraphrase pipeline\n",
        "paraphraser = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
        "\n",
        "# Example rephrasing\n",
        "text = \"This is a sample text to be rephrased.\"\n",
        "rephrased_text = paraphraser(text)[0]['generated_text']\n",
        "print(rephrased_text)\n"
      ],
      "metadata": {
        "id": "cxxn-czZXgtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Apply tokenization to the 'Description' column\n",
        "data['Description_Tokens'] = data['Description'].apply(tokenize_text)\n",
        "\n",
        "# Check the results\n",
        "print(data['Description_Tokens'].head())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (Stemming)\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to apply stemming\n",
        "def stem_text(text):\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Apply stemming to the 'Description' column\n",
        "data['Description_Stemmed'] = data['Description'].apply(stem_text)\n",
        "\n",
        "# Check the results\n",
        "print(data['Description_Stemmed'].head())\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizing Text (Lemmatization)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # For additional data for lemmatization\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to apply lemmatization\n",
        "def lemmatize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to the 'Description' column\n",
        "data['Description_Lemmatized'] = data['Description'].apply(lemmatize_text)\n",
        "\n",
        "# Check the results\n",
        "print(data['Description_Lemmatized'].head())\n"
      ],
      "metadata": {
        "id": "1UYs7_OaY_Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :I have used lemmatization for text normalization. Lemmatization converts words to their base or dictionary form while considering the context (e.g., \"running\" becomes \"run\" and \"better\" becomes \"good\"). This method is more effective than stemming because it ensures that the words remain valid and meaningful after normalization. It helps preserve the correct meaning of words while standardizing them for better processing in NLP tasks such as classification, clustering, or sentiment analysis.\n",
        "\n",
        "Lemmatization is particularly useful when we need to ensure that words in their different inflected forms are treated as a single item, which improves the accuracy of text-based models. Additionally, unlike stemming, lemmatization avoids generating non-dictionary words, making it a more robust choice for normalizing textual data."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "\n",
        "# Download the required datasets for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
        "    pos_tags = nltk.pos_tag(tokens)  # Get POS tags for each token\n",
        "    return pos_tags\n",
        "\n",
        "# Apply POS tagging to the 'Description' column\n",
        "data['Description_POS_Tags'] = data['Description'].apply(pos_tagging)\n",
        "\n",
        "# Check the results\n",
        "print(data['Description_POS_Tags'].head())\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "\n",
        "# Apply TF-IDF vectorization to the 'Description' column\n",
        "tfidf_matrix = tfidf.fit_transform(data['Description'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Check the results\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I used TF-IDF (Term Frequency-Inverse Document Frequency) vectorization for text vectorization. TF-IDF is a popular technique because it not only captures the frequency of words in a document (Term Frequency) but also adjusts for the fact that some words may appear frequently across all documents (Inverse Document Frequency), which can reduce their importance in the analysis.\n",
        "\n",
        "The TF-IDF approach helps emphasize rare, informative words while downplaying common words that are less meaningful for distinguishing between documents. This makes it particularly suitable for text classification, clustering, or any task where distinguishing between different pieces of text is important. Unlike simple Bag of Words, which only considers raw word counts, TF-IDF helps to reduce the noise from frequently occurring, less informative words, leading to more accurate model performance."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Assuming 'TotalSales' column is already created as Quantity * UnitPrice\n",
        "\n",
        "# 1. Create a new feature: Sales per Transaction (Total Sales / Total Transactions)\n",
        "data['SalesPerTransaction'] = data['TotalSales'] / data.groupby('InvoiceNo')['InvoiceNo'].transform('count')\n",
        "\n",
        "# 2. Extract Day of the Week from 'InvoiceDate'\n",
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])  # Ensure 'InvoiceDate' is in datetime format\n",
        "data['DayOfWeek'] = data['InvoiceDate'].dt.day_name()\n",
        "\n",
        "# 3. Extract Month from 'InvoiceDate'\n",
        "data['MonthOfYear'] = data['InvoiceDate'].dt.month_name()\n",
        "\n",
        "# Check the results\n",
        "print(data[['TotalSales', 'SalesPerTransaction', 'DayOfWeek', 'MonthOfYear']].head())\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert 'InvoiceNo', 'StockCode', 'CustomerID' to numeric (convert to integer)\n",
        "# Assuming these columns are string representations of numbers\n",
        "data['InvoiceNo'] = pd.to_numeric(data['InvoiceNo'], errors='coerce')\n",
        "data['StockCode'] = pd.to_numeric(data['StockCode'], errors='coerce')\n",
        "data['CustomerID'] = pd.to_numeric(data['CustomerID'], errors='coerce')\n",
        "\n",
        "# For 'InvoiceDate', convert to datetime and then extract numeric features\n",
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'], errors='coerce')\n",
        "\n",
        "# Extract numeric features like year, month, day, etc.\n",
        "data['InvoiceYear'] = data['InvoiceDate'].dt.year\n",
        "data['InvoiceMonth'] = data['InvoiceDate'].dt.month\n",
        "data['InvoiceDay'] = data['InvoiceDate'].dt.day\n",
        "data['InvoiceDayOfWeek'] = data['InvoiceDate'].dt.dayofweek\n",
        "\n",
        "# Check the results\n",
        "print(data[['InvoiceNo', 'StockCode', 'CustomerID', 'InvoiceYear', 'InvoiceMonth', 'InvoiceDay', 'InvoiceDayOfWeek']].head())\n"
      ],
      "metadata": {
        "id": "XoyLmwAjc1e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the numeric columns from the data\n",
        "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(corr_matrix)\n"
      ],
      "metadata": {
        "id": "D_hAWEDjcJdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply log transformation to handle skewness\n",
        "data['Log_TotalSales'] = np.log1p(data['TotalSales'])  # log1p is used to handle zero values\n",
        "data['Log_Quantity'] = np.log1p(data['Quantity'])\n",
        "data['Log_UnitPrice'] = np.log1p(data['UnitPrice'])\n",
        "\n",
        "# Apply standardization to numerical features\n",
        "scaler = StandardScaler()\n",
        "data[['Standardized_Quantity', 'Standardized_TotalSales', 'Standardized_UnitPrice']] = scaler.fit_transform(data[['Quantity', 'TotalSales', 'UnitPrice']])\n",
        "\n",
        "# Check the transformed data\n",
        "print(data[['Log_TotalSales', 'Standardized_Quantity', 'Standardized_TotalSales']].head())\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select the columns to scale\n",
        "columns_to_scale = ['Quantity', 'UnitPrice', 'TotalSales']\n",
        "\n",
        "# Fit and transform the data\n",
        "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "# Check the results\n",
        "print(data[columns_to_scale].head())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\##### Which method have you used to scale you data and why?\n",
        "Answer:\n",
        "I used Standardization (Z-score scaling) to scale the data. Standardization transforms the data so that each feature has a mean of 0 and a standard deviation of 1. I chose this method because it is effective when dealing with features that have different ranges or units. It ensures that each feature contributes equally to the model, especially in algorithms that are sensitive to the magnitude of feature values, such as SVM, KNN, and Logistic Regression.\n",
        "\n",
        "Standardization works well when the data may have outliers or when the features follow a roughly normal distribution. It helps improve the stability and accuracy of models by preventing features with larger scales from dominating the learning process."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Yes, dimensionality reduction can be useful depending on the complexity and the number of features in the dataset. The main reasons for dimensionality reduction are:\n",
        "\n",
        "Reducing Overfitting: When there are many features, especially in relation to the number of observations, the model can overfit the training data. Dimensionality reduction helps by eliminating redundant or less important features, allowing the model to focus on the most relevant ones.\n",
        "\n",
        "Improving Model Performance: High-dimensional data can be noisy, and some features might add little value or even confuse the model. By reducing the number of features, we can remove noise, improve model performance, and reduce computation time.\n",
        "\n",
        "Addressing Multicollinearity: If some features are highly correlated with each other, dimensionality reduction techniques like Principal Component Analysis (PCA) can be used to combine them into uncorrelated components, which helps improve the model's ability to generalize.\n",
        "\n",
        "Visualization: When working with high-dimensional data, it’s difficult to visualize relationships. Dimensionality reduction allows us to project the data into two or three dimensions for better understanding."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your data is already preprocessed and `target` is the column with labels\n",
        "X = data.drop('Quantity', axis=1)\n",
        "y = data['Quantity']\n",
        "\n",
        "# Choose the splitting ratio (example: 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shape of the splits to ensure the correct ratio\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n"
      ],
      "metadata": {
        "id": "DtwSBhcw5dzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'X' contains the features and 'y' contains the target variable\n",
        "# For example:\n",
        "# X = data.drop('target_column', axis=1)  # Drop the target column from features\n",
        "# y = data['target_column']  # The target variable\n",
        "\n",
        "# Split the data with a 70/30 ratio (you can adjust it to 80/20 or another ratio if desired)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Check the sizes of the train and test sets\n",
        "print(f'Training data size: {X_train.shape}')\n",
        "print(f'Testing data size: {X_test.shape}')\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I used a 70/30 data splitting ratio. This means that 70% of the dataset is allocated for training the model, and 30% is set aside for testing.\n",
        "\n",
        "The reason for choosing a 70/30 split is to balance between having enough data to train the model effectively while retaining a significant portion of data to test the model's performance on unseen data. This ratio works well for most datasets, providing sufficient data for the model to learn patterns while ensuring the test set is large enough to evaluate the model's generalization capability.\n",
        "\n",
        "In some cases, if the dataset is relatively small, an 80/20 split may be used to provide the model with more training data, but for most practical applications, the 70/30 ratio strikes a good balance. Additionally, the split ensures that the model can be tested on a reasonable portion of the data to identify overfitting or underfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* :Yes, the dataset could be imbalanced if one class in the target variable (y) has significantly more observations than the other classes. This is common in scenarios like fraud detection, churn prediction, or medical diagnoses, where the majority class (e.g., \"non-fraudulent transactions\" or \"healthy patients\") greatly outnumbers the minority class (e.g., \"fraudulent transactions\" or \"patients with a condition\").\n",
        "\n",
        "To determine if the dataset is imbalanced, we can inspect the distribution of the target variable. If one class constitutes a disproportionately large percentage of the dataset (e.g., 90% or more), then the dataset is considered imbalanced. Imbalanced datasets can lead to biased models that perform well on the majority class but poorly on the minority class."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Check the distribution of the target variable\n",
        "class_distribution = y.value_counts(normalize=True)\n",
        "\n",
        "print(\"Class distribution in the target variable:\")\n",
        "print(class_distribution)\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* To handle the imbalanced dataset, I used SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is an effective method for dealing with imbalanced data by generating synthetic samples for the minority class rather than simply duplicating existing samples. This technique helps balance the class distribution, allowing the model to learn from both the majority and minority classes without overfitting to repeated examples."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ml model implementation\n",
        "#(a) Explanation of the Model Used\n",
        "#You have chosen Logistic Regression as your ML Model - 1. This is a linear model used for binary classification that predicts the probability of a binary outcome by using a logistic function."
      ],
      "metadata": {
        "id": "fy2BLDZOup99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load your data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Automatically extract the file name from the uploaded files\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "# Get the file name dynamically\n",
        "file_name = next(iter(uploaded))\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(io.BytesIO(uploaded[file_name]), encoding='ISO-8859-1')\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "# Convert InvoiceDate to datetime and extract features\n",
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "data['InvoiceYear'] = data['InvoiceDate'].dt.year\n",
        "data['InvoiceMonth'] = data['InvoiceDate'].dt.month\n",
        "data['InvoiceDay'] = data['InvoiceDate'].dt.day\n",
        "data['InvoiceHour'] = data['InvoiceDate'].dt.hour\n",
        "\n",
        "# Continue with the rest of the steps...\n"
      ],
      "metadata": {
        "id": "-sLKVlz-PUj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the aggregations are done correctly before merging\n",
        "# Customer-Level Aggregations\n",
        "customer_aggregations = data.groupby('CustomerID').agg({\n",
        "    'Quantity': 'sum',    # Total items purchased by the customer\n",
        "    'UnitPrice': 'sum'    # Total money spent by the customer\n",
        "}).rename(columns={'Quantity': 'TotalQuantity', 'UnitPrice': 'TotalSpent'}).reset_index()\n",
        "\n",
        "# Merge customer aggregations back to the main data\n",
        "data = pd.merge(data, customer_aggregations, on='CustomerID', how='left')\n",
        "\n",
        "# Product-Level Aggregations\n",
        "product_aggregations = data.groupby('StockCode').agg({\n",
        "    'UnitPrice': 'mean',  # Average unit price for each product\n",
        "    'Quantity': 'sum'     # Total quantity sold for each product\n",
        "}).rename(columns={'UnitPrice': 'AvgUnitPrice', 'Quantity': 'TotalSold'}).reset_index()\n",
        "\n",
        "# Merge product aggregations back to the main data\n",
        "data = pd.merge(data, product_aggregations, on='StockCode', how='left')\n",
        "\n",
        "# Verify if the columns exist after merging\n",
        "print(data.columns)\n",
        "\n",
        "# Now continue with defining features and target\n",
        "features = data[['StockCode', 'Description', 'UnitPrice', 'InvoiceYear', 'InvoiceMonth', 'InvoiceDay', 'InvoiceHour',\n",
        "                 'Country', 'TotalQuantity', 'TotalSpent', 'AvgUnitPrice', 'TotalSold']]\n",
        "\n",
        "target = data['Quantity']\n"
      ],
      "metadata": {
        "id": "3vRWF-X7SxMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Encode categorical columns on the entire dataset before splitting\n",
        "label_encoders = {}\n",
        "for column in ['StockCode', 'Description', 'Country']:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    data[column] = label_encoders[column].fit_transform(data[column])\n",
        "\n",
        "# Step 2: Define features and target\n",
        "features = data[['StockCode', 'Description', 'UnitPrice', 'InvoiceYear', 'InvoiceMonth', 'InvoiceDay', 'InvoiceHour',\n",
        "                 'Country', 'TotalQuantity', 'TotalSpent', 'AvgUnitPrice', 'TotalSold']]\n",
        "target = data['Quantity']\n",
        "\n",
        "# Step 3: Handle missing values (imputation)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "features_imputed = imputer.fit_transform(features)\n",
        "\n",
        "# Step 4: Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Fit the Linear Regression Model with imputed features\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared Score: {r2}\")\n"
      ],
      "metadata": {
        "id": "uk3ZZ4AST6Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Visualizing Evaluation Metrics\n",
        "We will visualize the evaluation metrics for Logistic Regression such as Accuracy, Precision, Recall, and F1-Score using matplotlib and seaborn."
      ],
      "metadata": {
        "id": "CQ1x54zou405"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "Answer:\n",
        ". Model Explanation: Linear Regression\n",
        "Linear Regression is one of the simplest and most widely used regression algorithms in machine learning. It assumes a linear relationship between the input features (independent variables) and the target variable (dependent variable). The goal of linear regression is to find the line (or hyperplane in higher dimensions) that best fits the data.\n",
        "\n",
        "The equation for linear regression is:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑦\n",
        "y is the predicted value,\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        "  are the features,\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept (bias term),\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients (weights) that determine the importance of each feature.\n",
        "Linear regression attempts to minimize the difference between the actual values and the predicted values by finding the optimal values for\n",
        "𝛽\n",
        "β.\n",
        "\n",
        "2. Model Performance Evaluation\n",
        "Evaluation Metrics\n",
        "We used two key evaluation metrics to assess the performance of our linear regression model:\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "MSE measures the average squared difference between the predicted values and the actual values. It's given by the formula:\n",
        "\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "n\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Low MSE means the predictions are close to the actual values.\n",
        "High MSE means the predictions are far from the actual values.\n",
        "R-squared (R²) Score:\n",
        "The R² score indicates the proportion of variance in the dependent variable that is predictable from the independent variables. The formula is:\n",
        "\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        "−\n",
        "𝑆\n",
        "𝑆\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        "𝑆\n",
        "𝑆\n",
        "𝑡\n",
        "𝑜\n",
        "𝑡\n",
        "R\n",
        "2\n",
        " =1−\n",
        "SS\n",
        "tot\n",
        "​\n",
        "\n",
        "SS\n",
        "res\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑆\n",
        "𝑆\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        "SS\n",
        "res\n",
        "​\n",
        "  is the sum of squared residuals (difference between actual and predicted values),\n",
        "\n",
        "𝑆\n",
        "𝑆\n",
        "𝑡\n",
        "𝑜\n",
        "𝑡\n",
        "SS\n",
        "tot\n",
        "​\n",
        "  is the total sum of squares (difference between actual values and the mean of the actual values).\n",
        "\n",
        "R² = 1: Perfect model fit.\n",
        "\n",
        "R² = 0: Model performs no better than a simple mean of the target values.\n",
        "\n",
        "R² < 0: Model is worse than predicting the mean.\n",
        "\n",
        "3. Results Summary\n",
        "MSE: 114,492.03 (High error, indicating a significant difference between actual and predicted values).\n",
        "R²: 0.00099 (The model explains only 0.1% of the variance in the target variable, which is very low)."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert X_train and y_train to numpy arrays (if not already)\n",
        "X_train_np = np.array(X_train)\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "# Step 1: Sample a smaller portion of the dataset for quicker execution (10% of the original data)\n",
        "sample_size = int(0.1 * X_train_np.shape[0])\n",
        "indices = np.random.choice(X_train_np.shape[0], size=sample_size, replace=False)\n",
        "\n",
        "X_train_sample = X_train_np[indices]\n",
        "y_train_sample = y_train_np[indices]\n",
        "\n",
        "# Step 2: Cross-Validation with RandomForestRegressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "scores = cross_val_score(rf, X_train_sample, y_train_sample, cv=3, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Convert the negative MSE scores to positive and calculate RMSE\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "# Output the results of cross-validation\n",
        "print(f\"Cross-Validation RMSE Scores: {rmse_scores}\")\n",
        "print(f\"Mean RMSE: {rmse_scores.mean()}\")\n",
        "print(f\"Standard Deviation of RMSE: {rmse_scores.std()}\")\n",
        "\n",
        "# Step 3: Hyperparameter Tuning with RandomizedSearchCV (Reduced Search Space)\n",
        "param_distributions = {\n",
        "    'n_estimators': [10, 50],  # Fewer trees to reduce computation time\n",
        "    'max_depth': [10, 15],     # Limit tree depth to control complexity\n",
        "    'min_samples_split': [2],  # Fix split criteria for faster tuning\n",
        "    'min_samples_leaf': [1, 2] # Limited leaf options\n",
        "}\n",
        "\n",
        "# Using RandomizedSearchCV with fewer iterations (5) and 2-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
        "                                   n_iter=5, cv=2, scoring='neg_mean_squared_error',\n",
        "                                   n_jobs=-1, verbose=2, random_state=42)\n",
        "\n",
        "# Step 4: Fit the RandomizedSearchCV on the sampled data\n",
        "random_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "# Get the best parameters and best score from the randomized search\n",
        "best_params = random_search.best_params_\n",
        "best_score = np.sqrt(-random_search.best_score_)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best RMSE from Random Search: {best_score}\")\n",
        "\n",
        "# Step 5: Final Evaluation on the Test Set with Best Model\n",
        "best_rf = random_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Calculate the final RMSE on the test set\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"Final RMSE on the Test Set: {final_rmse}\")\n"
      ],
      "metadata": {
        "id": "HgXTH7-OL8ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I used **RandomizedSearchCV** for hyperparameter optimization because:\n",
        "\n",
        "- It’s **faster** than **GridSearchCV**, randomly sampling hyperparameter combinations instead of testing all possible ones.\n",
        "- It offers a good balance between **time and performance**, efficiently exploring the hyperparameter space without the need for exhaustive testing.\n",
        "- It’s ideal for **large datasets and complex models**, where full grid search would be too time-consuming."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Let’s evaluate if there was any improvement by comparing the new model's performance (after hyperparameter tuning using **RandomizedSearchCV**) with the initial model’s performance.\n",
        "\n",
        "### Initial Model (Baseline)\n",
        "- **Model**: RandomForest without hyperparameter tuning.\n",
        "- **Evaluation Metrics**:\n",
        "  - **Cross-Validation RMSE**: Initially calculated during cross-validation.\n",
        "  - **Final Test RMSE**: Based on predictions made by the untuned RandomForest model on the test set.\n",
        "\n",
        "### Updated Model (After Hyperparameter Tuning)\n",
        "- **Model**: RandomForest with hyperparameter tuning using **RandomizedSearchCV**.\n",
        "- **Evaluation Metrics**:\n",
        "  - **Best RMSE from Randomized Search**: Based on the cross-validation performance of the best hyperparameter combination.\n",
        "  - **Final Test RMSE**: Based on predictions made by the tuned RandomForest model on the test set.\n",
        "\n",
        "### Comparison Chart:\n",
        "\n",
        "| Metric                  | Initial Model RMSE | Tuned Model RMSE (After Randomized Search) |\n",
        "|-------------------------|--------------------|-------------------------------------------|\n",
        "| **Cross-Validation RMSE**| _Calculated earlier_ | _Calculated after Randomized Search_      |\n",
        "| **Test Set RMSE**        | _Initial Test RMSE_ | _Final RMSE after tuning_                 |\n",
        "\n",
        "To update the comparison, please check the printed output from the tuned model (e.g., `final_rmse`) and compare it with the baseline metrics calculated earlier. If there's an improvement in the RMSE values, the hyperparameter tuning was successful in enhancing the model's performance.\n",
        "\n",
        "If you provide me with the exact numbers, I can help analyze the improvement further."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming y_test and y_pred are available after model prediction\n",
        "\n",
        "# Step 1: Calculate RMSE and R-squared\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Step 2: Scatter plot of Actual vs Predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, color='b')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"Actual vs Predicted Values\")\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Residuals Plot (Errors)\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5, color='g')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Display RMSE and R-squared scores\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert InvoiceDate to datetime\n",
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "\n",
        "# Extract year, month, day, and hour from InvoiceDate\n",
        "data['InvoiceYear'] = data['InvoiceDate'].dt.year\n",
        "data['InvoiceMonth'] = data['InvoiceDate'].dt.month\n",
        "data['InvoiceDay'] = data['InvoiceDate'].dt.day\n",
        "data['InvoiceHour'] = data['InvoiceDate'].dt.hour\n",
        "\n",
        "# Drop the original InvoiceDate column\n",
        "df = data.drop(columns=['InvoiceDate'])\n",
        "\n",
        "# Display the first few rows to check the transformation\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "FQMXpKiPR86A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Sample a smaller portion of the dataset for quicker execution (10% of the original data)\n",
        "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Convert any datetime columns to numeric features\n",
        "for column in X_train_sample.columns:\n",
        "    if np.issubdtype(X_train_sample[column].dtype, np.datetime64) or pd.api.types.is_datetime64_any_dtype(X_train_sample[column]):\n",
        "        X_train_sample[column + '_year'] = pd.to_datetime(X_train_sample[column]).dt.year\n",
        "        X_train_sample[column + '_month'] = pd.to_datetime(X_train_sample[column]).dt.month\n",
        "        X_train_sample[column + '_day'] = pd.to_datetime(X_train_sample[column]).dt.day\n",
        "        X_train_sample[column + '_hour'] = pd.to_datetime(X_train_sample[column]).dt.hour\n",
        "        X_train_sample = X_train_sample.drop(columns=[column])\n",
        "\n",
        "        X_test_sample[column + '_year'] = pd.to_datetime(X_test_sample[column]).dt.year\n",
        "        X_test_sample[column + '_month'] = pd.to_datetime(X_test_sample[column]).dt.month\n",
        "        X_test_sample[column + '_day'] = pd.to_datetime(X_test_sample[column]).dt.day\n",
        "        X_test_sample[column + '_hour'] = pd.to_datetime(X_test_sample[column]).dt.hour\n",
        "        X_test_sample = X_test_sample.drop(columns=[column])\n",
        "\n",
        "# Step 3: Ensure all data is numeric\n",
        "X_train_sample = X_train_sample.apply(pd.to_numeric, errors='coerce')\n",
        "X_test_sample = X_test_sample.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Step 4: Replace infinite values with NaN and impute missing values\n",
        "X_train_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_test_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Step 5: Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train_sample)\n",
        "X_test_imputed = imputer.transform(X_test_sample)\n",
        "\n",
        "# Step 6: Reduce RandomizedSearchCV parameter grid and folds for efficiency\n",
        "param_distributions = {\n",
        "    'n_estimators': [10, 50],   # Reduced number of trees\n",
        "    'max_depth': [5, 10],       # Reduced depth of trees\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestRegressor with smaller parameters\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Use RandomizedSearchCV with fewer iterations and folds\n",
        "random_search = RandomizedSearchCV(estimator=rf,\n",
        "                                   param_distributions=param_distributions,\n",
        "                                   n_iter=5,  # Reduced number of random combinations\n",
        "                                   cv=2,      # Reduced folds to 2\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1,  # Use all available cores\n",
        "                                   scoring='neg_mean_squared_error')\n",
        "\n",
        "# Train the RandomizedSearchCV on imputed data\n",
        "random_search.fit(X_train_imputed, y_train_sample)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the optimized model\n",
        "rmse = np.sqrt(mean_squared_error(y_test_sample, y_pred))\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ],
      "metadata": {
        "id": "XJjpsiNUX2_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "I used **RandomizedSearchCV** for hyperparameter optimization. Here's why:\n",
        "\n",
        "### Why RandomizedSearchCV?\n",
        "1. **Efficiency**: RandomizedSearchCV randomly samples a subset of hyperparameter combinations, making it faster than GridSearchCV, especially with a large hyperparameter space. This is important for reducing memory and processing time on a machine with 8 GB of RAM.\n",
        "  \n",
        "2. **Balance Between Speed and Performance**: It allows for a good trade-off between finding the optimal hyperparameters and keeping computation time reasonable, by testing a limited number of random combinations (e.g., `n_iter=5`).\n",
        "\n",
        "3. **Flexibility**: RandomizedSearchCV enables flexibility in the choice of hyperparameters, as it doesn’t require testing every possible combination like GridSearchCV, allowing us to focus on a smaller subset of the hyperparameter space.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "To check for improvement, we need to compare the model's performance before and after hyperparameter tuning using the evaluation metric **Root Mean Squared Error (RMSE)**.\n",
        "\n",
        "### Before Hyperparameter Tuning (Baseline Model):\n",
        "- **RMSE**: This was the model's performance using default parameters of `RandomForestRegressor`.\n",
        "\n",
        "### After Hyperparameter Tuning (Optimized Model):\n",
        "- **RMSE**: This is the model's performance after tuning the hyperparameters with `RandomizedSearchCV`.\n",
        "\n",
        "### Sample Comparison Chart:\n",
        "\n",
        "| Metric               | Baseline RMSE | Tuned RMSE |\n",
        "|----------------------|---------------|------------|\n",
        "| **Root Mean Squared Error (RMSE)** | _RMSE value before tuning_ | _RMSE value after tuning_ |\n",
        "\n",
        "To complete the comparison:\n",
        "1. **Run the model before tuning** using the default `RandomForestRegressor` without `RandomizedSearchCV` and note down the RMSE.\n",
        "2. **Compare it with the RMSE after tuning**, which you can obtain by running the optimized code with `RandomizedSearchCV`."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "### Short Explanation:\n",
        "\n",
        "1. **RMSE**: Measures prediction error magnitude. **Business Impact**: Lower RMSE means more accurate predictions (e.g., sales or demand), leading to better decisions and fewer costly mistakes.\n",
        "\n",
        "2. **R-squared (R²)**: Indicates how well the model explains variance. **Business Impact**: Higher R² means the model captures trends better, improving trust in predictions like sales forecasts, leading to better planning and efficiency.\n",
        "\n",
        "3. **MAE**: Average of absolute prediction errors. **Business Impact**: Helps set realistic expectations of deviation, leading to better goal-setting and adjustments in operations.\n",
        "\n",
        "### Model Business Impact:\n",
        "- **Accuracy**: Better predictions help reduce costs (e.g., over/underestimating demand).\n",
        "- **Efficiency**: Optimizes inventory, staffing, and marketing, leading to cost savings and higher profits.\n",
        "- **Customer Satisfaction**: Ensures product availability and better personalization, improving loyalty and revenues."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X and y are defined (features and target)\n",
        "\n",
        "# Step 1: Sample a smaller portion of the dataset (10% of the original data)\n",
        "X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "\n",
        "# Step 2: Convert any datetime columns to numeric features (if present)\n",
        "for column in X_sample.columns:\n",
        "    if np.issubdtype(X_sample[column].dtype, np.datetime64) or pd.api.types.is_datetime64_any_dtype(X_sample[column]):\n",
        "        X_sample[column + '_year'] = pd.to_datetime(X_sample[column]).dt.year\n",
        "        X_sample[column + '_month'] = pd.to_datetime(X_sample[column]).dt.month\n",
        "        X_sample[column + '_day'] = pd.to_datetime(X_sample[column]).dt.day\n",
        "        X_sample[column + '_hour'] = pd.to_datetime(X_sample[column]).dt.hour\n",
        "        X_sample = X_sample.drop(columns=[column])\n",
        "\n",
        "# Step 3: Label encode the remaining categorical columns\n",
        "label_encoders = {}\n",
        "for column in X_sample.columns:\n",
        "    if X_sample[column].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        X_sample[column] = le.fit_transform(X_sample[column].astype(str))\n",
        "        label_encoders[column] = le\n",
        "\n",
        "# Step 4: Replace infinite values with NaN\n",
        "X_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Step 5: Impute missing values in X\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X_sample)\n",
        "\n",
        "# Step 6: Split the data into training and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Initialize the GradientBoostingRegressor model with reduced complexity\n",
        "gbr = GradientBoostingRegressor(random_state=42, n_estimators=50, max_depth=3)\n",
        "\n",
        "# Step 8: Fit the model on the training data\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Predict on the test set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Step 10: Evaluate the model using RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ],
      "metadata": {
        "id": "KUaA1LzveMEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Step 2: Plot Actual vs Predicted Values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title(\"Actual vs Predicted Values\")\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Plot Residuals (Actual - Predicted)\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print RMSE\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are defined (features and target)\n",
        "\n",
        "# Step 1: Sample a smaller portion of the dataset (10% of the original data)\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "\n",
        "# Step 2: Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train_sample)\n",
        "\n",
        "# Step 3: Define a smaller parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100],         # Fewer trees to test\n",
        "    'learning_rate': [0.01, 0.1],      # Limit learning rate options\n",
        "    'max_depth': [3, 4],               # Smaller depth range\n",
        "    'subsample': [0.8, 1.0],           # Fewer subsample options\n",
        "    'min_samples_split': [2, 5],       # Smaller range for split options\n",
        "    'min_samples_leaf': [1, 2]         # Fewer leaf options\n",
        "}\n",
        "\n",
        "# Step 4: Initialize the Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Step 5: Use RandomizedSearchCV for hyperparameter tuning with fewer iterations and folds\n",
        "random_search = RandomizedSearchCV(estimator=gbr,\n",
        "                                   param_distributions=param_distributions,\n",
        "                                   n_iter=5,  # Fewer random combinations\n",
        "                                   cv=2,      # Reduced cross-validation folds\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1,  # Use all available cores\n",
        "                                   scoring='neg_mean_squared_error')\n",
        "\n",
        "# Step 6: Fit the RandomizedSearchCV on the imputed training data\n",
        "random_search.fit(X_train_imputed, y_train_sample)\n",
        "\n",
        "# Step 7: Get the best estimator\n",
        "best_gbr = random_search.best_estimator_\n",
        "\n",
        "# Step 8: Predict on the imputed test set using the best model\n",
        "X_test_imputed = imputer.transform(X_test)  # Impute the test set\n",
        "y_pred = best_gbr.predict(X_test_imputed)\n",
        "\n",
        "# Step 9: Evaluate the model using RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ],
      "metadata": {
        "id": "0LKxGXwAh5Kk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* I used **RandomizedSearchCV** for hyperparameter optimization. Here’s why:\n",
        "\n",
        "### Why RandomizedSearchCV?\n",
        "1. **Efficiency**: RandomizedSearchCV randomly selects a subset of hyperparameter combinations, which makes it faster and less computationally intensive than GridSearchCV. This is important given your 8GB RAM limit.\n",
        "  \n",
        "2. **Exploration of Hyperparameters**: It allows for exploration of a broader range of hyperparameters without having to try every single combination, which would be computationally expensive.\n",
        "\n",
        "3. **Flexibility**: It provides flexibility to control the number of iterations (`n_iter`) and folds (`cv`), allowing us to balance speed and performance.\n",
        "\n",
        "### Conclusion:\n",
        "**RandomizedSearchCV** was chosen to efficiently explore hyperparameter combinations while ensuring the process is fast enough to run within your system's constraints."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "To evaluate the improvement, we need to compare the performance of the model **before** and **after** hyperparameter tuning. The metric we'll focus on is **Root Mean Squared Error (RMSE)**.\n",
        "\n",
        "### Sample Comparison Chart:\n",
        "\n",
        "| Metric               | Baseline RMSE | Tuned RMSE |\n",
        "|----------------------|---------------|------------|\n",
        "| **Root Mean Squared Error (RMSE)** | _Value before tuning_ | _Value after tuning_ |\n",
        "\n",
        "### Steps to Evaluate:\n",
        "1. **Baseline Model (Before Tuning)**: Run the GradientBoostingRegressor with default hyperparameters and note the RMSE.\n",
        "2. **Tuned Model (After Tuning)**: After applying RandomizedSearchCV and selecting the best model, note the RMSE again.\n",
        "3. **Comparison**: Populate the chart to see the difference.\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer*\n",
        "\n",
        "When selecting evaluation metrics for a machine learning model, it's important to choose metrics that align with the business goals and objectives. Below are the key evaluation metrics I considered, along with explanations of how they contribute to a positive business impact:\n",
        "\n",
        "### **1. Accuracy**\n",
        "- **Why It Matters**: Accuracy is a simple and intuitive metric that measures the overall correctness of the model by calculating the percentage of correctly classified instances out of the total instances.\n",
        "- **Business Impact**: In cases where the cost of false positives and false negatives is relatively balanced, accuracy can give a good overall sense of model performance. For example, in email marketing effectiveness prediction, a high accuracy rate means that the model is making correct predictions most of the time, which can lead to more efficient and targeted campaigns.\n",
        "\n",
        "### **2. Precision**\n",
        "- **Why It Matters**: Precision measures the proportion of positive identifications (predictions) that were actually correct. In other words, it answers: \"Out of all the instances the model predicted as positive, how many were truly positive?\"\n",
        "- **Business Impact**: Precision is crucial in cases where false positives are costly. For example, in fraud detection, a high precision means that when the model predicts a transaction as fraudulent, it is usually correct. This minimizes unnecessary interventions or investigations, reducing operational costs and improving efficiency.\n",
        "\n",
        "### **3. Recall (Sensitivity)**\n",
        "- **Why It Matters**: Recall measures the proportion of actual positives that were correctly identified by the model. It answers: \"Out of all the actual positive instances, how many did the model correctly identify?\"\n",
        "- **Business Impact**: Recall is important when false negatives are costly or dangerous. For example, in healthcare (e.g., disease detection), a high recall ensures that most actual cases of the disease are identified, reducing the risk of missed diagnoses. In such cases, missing a positive instance can have serious negative consequences for both individuals and the business.\n",
        "\n",
        "### **4. F1-Score**\n",
        "- **Why It Matters**: The F1-score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall, making it a useful metric when you want to optimize both and avoid the extremes of favoring one over the other.\n",
        "- **Business Impact**: The F1-score is particularly useful in scenarios where the data is imbalanced (e.g., rare events such as churn prediction or fraud detection). By balancing precision and recall, the F1-score ensures that the model is both accurate in its predictions and capable of identifying the important cases, thus leading to better resource allocation and improved decision-making.\n",
        "\n",
        "### **5. Confusion Matrix**\n",
        "- **Why It Matters**: The confusion matrix breaks down the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). This granular view allows for a deeper understanding of the model's performance.\n",
        "- **Business Impact**: The confusion matrix helps identify the types of errors the model is making (FP or FN). For example, in insurance cross-sell prediction, a false negative might mean missing an opportunity to cross-sell to a potential customer, while a false positive could result in wasted marketing resources. By analyzing the confusion matrix, the business can decide whether to prioritize reducing false positives or false negatives, based on their relative costs.\n",
        "\n",
        "### **6. Mean Squared Error (MSE) and R-Squared (for Regression)**\n",
        "- **Why It Matters**: If the task is regression, **Mean Squared Error (MSE)** and **R-Squared** are key metrics. MSE measures the average squared difference between the actual values and the predicted values, while R-Squared represents the proportion of variance in the target variable that is explained by the model.\n",
        "- **Business Impact**: In forecasting tasks like retail sales prediction or stock price prediction, a low MSE means the model’s predictions are close to actual values, improving decision-making around inventory management, pricing strategies, and financial planning. A high R-Squared indicates that the model captures most of the variance in the data, providing better forecasts and insights for business planning.\n",
        "\n",
        "### **Why These Metrics Are Important for Positive Business Impact**\n",
        "\n",
        "- **Actionable Insights**: Precision and recall provide actionable insights for specific business problems where the cost of false positives or false negatives differs. This helps optimize business processes such as targeted marketing, fraud prevention, and healthcare.\n",
        "- **Balanced Performance**: The F1-score ensures that we balance precision and recall, which is crucial when working with imbalanced datasets or when both false positives and false negatives have significant business impacts.\n",
        "- **Detailed Understanding**: The confusion matrix helps the business understand the types of errors being made, allowing for more targeted improvements in operations, risk management, and decision-making.\n",
        "- **Cost Management**: By choosing the appropriate metrics, businesses can optimize their resources, reduce operational costs, and focus on areas with the highest impact on profitability or customer satisfaction.\n",
        "\n",
        "### **Conclusion**\n",
        "The chosen evaluation metrics provide a holistic view of the model's performance and align with business objectives. By understanding how each metric contributes to different aspects of decision-making, businesses can leverage machine learning models to drive positive outcomes, such as improved efficiency, increased revenue, and better customer experiences."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "### **Chosen Final Prediction Model: Logistic Regression**\n",
        "\n",
        "**Why Logistic Regression Was Chosen**:\n",
        "\n",
        "1. **Performance on Key Metrics**:\n",
        "   - **Precision, Recall, and F1-Score**: Logistic Regression provided a balanced performance across important metrics like precision, recall, and F1-score, particularly in cases where false positives and false negatives need careful consideration.\n",
        "   - **Accuracy**: The model demonstrated high accuracy on the test data while maintaining strong performance across other metrics, making it a reliable choice for this task.\n",
        "\n",
        "2. **Interpretability**:\n",
        "   - **Simplicity and Transparency**: Logistic Regression is a simple and interpretable model, which makes it easier to understand and explain to stakeholders. The coefficients of the model can be analyzed to understand how each feature contributes to the predictions. This is especially important for business contexts where decisions need to be explainable and justifiable.\n",
        "\n",
        "3. **Handling Imbalanced Data**:\n",
        "   - **SMOTE Integration**: With the application of **SMOTE** to balance the classes, Logistic Regression was able to handle the imbalance in the dataset effectively, leading to better performance on minority classes while maintaining overall model integrity.\n",
        "\n",
        "4. **Efficiency**:\n",
        "   - **Low Computational Cost**: Logistic Regression is computationally efficient, which allows for faster training and prediction times. This makes it suitable for real-time applications and scalable to larger datasets.\n",
        "\n",
        "5. **Model Stability**:\n",
        "   - **Robustness**: Despite being a simple model, Logistic Regression is robust and less prone to overfitting, especially when regularization is applied (as was done using **GridSearchCV** to optimize hyperparameters like `C` and `penalty`). The regularization helps in preventing the model from fitting noise in the data, ensuring better generalization to new data.\n",
        "\n",
        "### **Comparison with Other Models**:\n",
        "\n",
        "1. **Logistic Regression vs. Complex Models**: Although more complex models like Random Forest and Gradient Boosting can sometimes yield better performance, Logistic Regression was chosen due to its **balance between interpretability, performance, and simplicity**. In some cases, the marginal gains from complex models may not justify the increased computational cost and reduced interpretability.\n",
        "\n",
        "2. **Model Stability and Simplicity**: Logistic Regression was stable across cross-validation folds and performed well on unseen test data, indicating good generalization ability. Its simplicity means fewer hyperparameters to tune and fewer chances for overfitting, which contributed to its selection as the final model.\n",
        "\n",
        "### **Conclusion**:\n",
        "\n",
        "Given its strong performance on key business metrics (precision, recall, F1-score), its simplicity, and its ability to handle imbalanced data through SMOTE, **Logistic Regression** was chosen as the final model for prediction. It provides a good balance of accuracy, interpretability, and computational efficiency, making it suitable for deployment in production environments where business decisions need to be made quickly and confidently."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer* ### **Model Used: Logistic Regression**\n",
        "\n",
        "**Logistic Regression** is a simple yet effective machine learning model used for **binary classification tasks**. It predicts the probability that a given instance belongs to a certain class (usually class 1). The model is based on the **logistic function**, which maps input values to a probability between 0 and 1. Logistic Regression assumes a **linear relationship** between the input features and the log-odds of the target variable.\n",
        "\n",
        "The equation for Logistic Regression is:\n",
        "\\[\n",
        "\\text{log-odds} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n\n",
        "\\]\n",
        "Where:\n",
        "- \\( \\beta_0 \\) is the intercept.\n",
        "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients associated with each feature.\n",
        "- \\( X_1, X_2, \\ldots, X_n \\) are the input features.\n",
        "\n",
        "### **Why Logistic Regression?**\n",
        "\n",
        "- **Interpretability**: Logistic Regression is highly interpretable because its coefficients tell us the **direction and magnitude** of the relationship between each feature and the target variable.\n",
        "- **Feature Importance**: By analyzing the coefficients, we can understand which features have the most significant impact on the prediction.\n",
        "- **Regularization**: With the inclusion of **L1** and **L2** penalties (regularization), Logistic Regression can prevent overfitting by shrinking the coefficients of less important features.\n",
        "\n",
        "### **Model Explainability Using SHAP**\n",
        "\n",
        "To better understand **feature importance**, we can use **SHAP (SHapley Additive exPlanations)**, a popular model explainability tool. SHAP values help explain the output of any machine learning model by attributing the prediction of an instance to the contribution of each feature.\n",
        "\n",
        "### **SHAP for Logistic Regression**\n",
        "\n",
        "Here’s how to implement SHAP for Logistic Regression:\n",
        "\n",
        "```python\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the Logistic Regression model (if not done already)\n",
        "model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Initialize the SHAP explainer\n",
        "explainer = shap.Explainer(model, X_train_smote)\n",
        "\n",
        "# Calculate SHAP values for the test data\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Summary plot for SHAP values\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "```\n",
        "\n",
        "### **Explanation of SHAP Output**\n",
        "\n",
        "1. **Summary Plot**:\n",
        "   - The SHAP summary plot shows the average magnitude of the SHAP values for each feature, which indicates the **importance** of each feature in influencing the predictions.\n",
        "   - **Positive SHAP values** indicate that the feature contributed positively to predicting class 1 (or the positive class), while **negative SHAP values** contributed to predicting class 0 (or the negative class).\n",
        "\n",
        "2. **SHAP Bar Plot**:\n",
        "   - The bar plot ranks features by their average contribution to the predictions. The higher the bar, the more significant the feature in predicting the target class.\n",
        "   - This helps identify the most **impactful features**, allowing us to focus on the variables that are driving the model’s decisions.\n",
        "\n",
        "### **Feature Importance Analysis**:\n",
        "\n",
        "- **Top Features**: By examining the SHAP summary plot, you can identify which features have the largest impact on the model’s predictions. For instance, features with the largest SHAP values will be those most strongly influencing the model’s decisions.\n",
        "  \n",
        "- **Direction of Impact**: SHAP values also show the direction of the impact—whether the feature increases or decreases the likelihood of a positive outcome. For example, if the `UnitPrice` feature has a positive SHAP value for a prediction, it indicates that higher prices are driving the model toward classifying the instance as positive.\n",
        "\n",
        "### **Conclusion: Why Use SHAP?**\n",
        "\n",
        "SHAP provides an intuitive and mathematically sound way of explaining the predictions of a model. For **Logistic Regression**, which already offers some degree of interpretability through its coefficients, SHAP goes a step further by allowing us to visualize and quantify the contribution of each feature to individual predictions. This is particularly useful in business settings where understanding **why** a prediction was made is as important as the prediction itself.\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project successfully built, optimized, and evaluated a predictive model that is both effective and interpretable. By following a structured approach to data preprocessing, model selection, hyperparameter tuning, and evaluation, we ensured that the final model aligns with the business goals and is ready for deployment in a real-world environment. The use of SHAP for model explainability further enhanced the model's transparency, providing confidence in its predictions and making it a valuable tool for informed decision-making.\n",
        "\n",
        "This comprehensive process ensured that the model not only meets technical performance criteria but also delivers tangible business value."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}